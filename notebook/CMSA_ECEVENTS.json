{
	"name": "CMSA_ECEVENTS",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pbixpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c7d66596-9d1a-4b1c-a825-7fc6443f24b1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/095c0f82-6a29-4db3-9f53-6808ff55fa13/resourceGroups/NavisBIAnalyticsPowerBiData2/providers/Microsoft.Synapse/workspaces/navispowerbidata2/bigDataPools/pbixpool",
				"name": "pbixpool",
				"type": "Spark",
				"endpoint": "https://navispowerbidata2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pbixpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, year, month, weekofyear, to_timestamp, row_number\n",
					"from pyspark.sql.window import Window"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# === Spark session ===\n",
					"spark = SparkSession.builder.appName(\"ECEVENTS_Transform\").getOrCreate()\n",
					"\n",
					"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
					"spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
					"\n",
					"\n",
					"# Configure storage credentials (if needed)\n",
					"# spark.conf.set(\n",
					"#     \"fs.azure.account.key.navispowerbidatastorage2.dfs.core.windows.net\",\n",
					"#     \"<storage-account-key>\"\n",
					"# )\n",
					"\n",
					"# Paths\n",
					"input_path   = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/bronze/ecevents/load/\"\n",
					"output_path  = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/silver/ecevents/\"\n",
					"archive_path = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/bronze/ecevents/archive/\"\n",
					"\n",
					"# Updated headers\n",
					"headers = [\n",
					"    \"ecEventGkey\",\"yardGkey\",\"YardID\",\"FcyID\",\"cheKind\",\"CHE_Name\",\"CHE_ID\",\"TimeStamp\",\n",
					"    \"eventType\",\"eventSubType\",\"eventTypeDescription\",\"fromCheIdName\",\"ToCheIdName\",\"unit\",\n",
					"    \"pow\",\"pool\",\"workQueue\",\"travelDistance\",\"moveKind\",\"isTwin\",\"startDistance\",\n",
					"    \"workAssignment\",\"unitRef\",\"tran_ID\",\"locType\",\"locID\",\"locSlot\",\"opsPosID\",\n",
					"    \"unladenLoctype\",\"unladenLocID\",\"unladenLocSlot\",\"ladenLoctype\",\"ladenLocID\",\n",
					"    \"laden_LocSlot\",\"lastEstMoveTime\",\"operator_name\"\n",
					"]\n",
					"\n",
					"# Read CSV without header\n",
					"df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \",\").csv(input_path)\n",
					"\n",
					"# Apply updated headers\n",
					"df = df.toDF(*headers)\n",
					"\n",
					"# Add constant client_id\n",
					"df = df.withColumn(\"client_id\", lit(\"CMSA\"))\n",
					"\n",
					"# Convert TimeStamp to proper timestamp\n",
					"df = df.withColumn(\"EcEventTime\", to_timestamp(col(\"TimeStamp\"), \"dd-MM-yyyy HH:mm:ss\"))\n",
					"\n",
					"# Derive date parts\n",
					"df = df.withColumn(\"Year\", year(col(\"EcEventTime\"))) \\\n",
					"       .withColumn(\"Month\", month(col(\"EcEventTime\"))) \\\n",
					"       .withColumn(\"Week\", weekofyear(col(\"EcEventTime\")))\n",
					"\n",
					"# Window to get LAST row by EcEventTime\n",
					"windowSpec = Window.partitionBy(\"ecEventGkey\").orderBy(col(\"EcEventTime\").desc())\n",
					"\n",
					"df_last = df.withColumn(\"rn\", row_number().over(windowSpec)) \\\n",
					"            .filter(col(\"rn\") == 1) \\\n",
					"            .drop(\"rn\")\n",
					"\n",
					"# Write to Delta\n",
					"# df_last.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
					"\n",
					"\n",
					"# Check if Delta table already exists at the path\n",
					"if DeltaTable.isDeltaTable(spark, output_path):\n",
					"    delta_table = DeltaTable.forPath(spark, output_path)\n",
					"\n",
					"    (\n",
					"        delta_table.alias(\"t\")\n",
					"        .merge(\n",
					"            df.alias(\"s\"),\n",
					"            \"t.ecEventGkey = s.ecEventGkey\"  # match condition\n",
					"        )\n",
					"        .whenMatchedUpdateAll()  # update all columns on match\n",
					"        .whenNotMatchedInsertAll()  # insert new row when not matched\n",
					"        .execute()\n",
					"    )\n",
					"else:\n",
					"    # First-time write\n",
					"    (\n",
					"        df.write\n",
					"        .format(\"delta\")\n",
					"        .mode(\"overwrite\")\n",
					"        .save(output_path)\n",
					"    )\n",
					"\n",
					"# Move processed files to dated archive\n",
					"# input_files = dbutils.fs.ls(input_path)\n",
					"# for file_info in input_files:\n",
					"#     file_name = file_info.name\n",
					"#     dbutils.fs.mv(file_info.path, archive_path + file_name)\n",
					"\n",
					"print(\"âœ… Transformation complete with updated headers, client_id added, and files archived.\")\n",
					""
				],
				"execution_count": null
			}
		]
	}
}