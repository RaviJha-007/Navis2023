{
	"name": "CMSA_MOVES",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "pbixpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4a81a548-9c44-4cf2-a677-65474cd612e2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/095c0f82-6a29-4db3-9f53-6808ff55fa13/resourceGroups/NavisBIAnalyticsPowerBiData2/providers/Microsoft.Synapse/workspaces/navispowerbidata2/bigDataPools/pbixpool",
				"name": "pbixpool",
				"type": "Spark",
				"endpoint": "https://navispowerbidata2.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pbixpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, year, month, weekofyear, to_timestamp, row_number\n",
					"from pyspark.sql.window import Window\n",
					"\n",
					"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql import SparkSession, Window\n",
					"\n",
					"spark = SparkSession.builder.getOrCreate()\n",
					"\n",
					"# Paths\n",
					"input_path   = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/bronze/moves/load/\"\n",
					"output_path  = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/silver/moves/\"\n",
					"archive_path = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/bronze/moves/archive/\"\n",
					"\n",
					"\n",
					"# Load CSVs (header = True/False depending on files)\n",
					"df_all = (\n",
					"    spark.read\n",
					"    .option(\"header\", True)  # Will be False if no header in CSV\n",
					"    .option(\"inferSchema\", True)\n",
					"    .csv(input_path)\n",
					")\n",
					"\n",
					"# Force header names (because some files have no headers)\n",
					"new_headers = [\n",
					"    \"Unit Nbr\",\"Facility Id\",\"Complex Id\",\"Yard Id\",\"Carrier Visit Key\",\"Event Type Id\",\"Move Kind\",\n",
					"    \"From Location\",\"To Location\",\"From Position Name\",\"To Position Name\",\"Carry CHE Operator Id\",\n",
					"    \"Fetch CHE Operator Id\",\"Put CHE Operator Id\",\"Fetch CHE Id\",\"Fetch CHE Kind\",\"Carry CHE Id\",\n",
					"    \"Carry CHE Kind\",\"Put CHE Id\",\"Put CHE Kind\",\"QC CHE Id\",\"QC CHE Kind\",\"t_carry_complete\",\"t_put\",\n",
					"    \"t_fetch\",\"t_dispatch\",\"t_discharge\",\"t_carry_dispatch\",\"t_carry_fetch_ready\",\"t_carry_put_ready\",\n",
					"    \"dist_carry\",\"dist_start\",\"Twin Fetch\",\"Twin Carry\",\"Twin Put\",\"Category Id\",\"Freight Kind Id\",\n",
					"    \"to_pos_gkey\",\"fm_pos_gkey\",\"Restow Reason\",\"Container Line Id\",\"Container Line Role\",\"pow\",\n",
					"    \"mve_gkey\",\"ufv_gkey\",\"Berth Id\",\"Goods And Ctr Wt Kg\",\"active_ufv\",\"Requires Power Flag\",\"OOG Flag\",\n",
					"    \"Shipper Id\",\"Consignee Id\",\"Commodity Id\",\"ATA\",\"ATD\",\"Rehandles\"\n",
					"]\n",
					"\n",
					"df_all = df_all.toDF(*new_headers)"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"# Convert timestamp columns\n",
					"timestamp_cols = [\n",
					"    \"t_carry_complete\",\"t_put\",\"t_fetch\",\"t_dispatch\",\"t_discharge\",\n",
					"    \"t_carry_dispatch\",\"t_carry_fetch_ready\",\"t_carry_put_ready\",\"ATA\",\"ATD\"\n",
					"]\n",
					"for col_name in timestamp_cols:\n",
					"    df_all = df_all.withColumn(col_name, F.to_timestamp(col_name, \"yyyy-MM-dd HH:mm:ss\"))\n",
					"\n",
					"# Derive Year, Month, Week from t_put\n",
					"df_all = (\n",
					"    df_all.withColumn(\"Year\", F.year(\"t_put\"))\n",
					"          .withColumn(\"Month\", F.month(\"t_put\"))\n",
					"          .withColumn(\"Week\", F.weekofyear(\"t_put\"))\n",
					")\n",
					"\n",
					"# Latest record per mve_gkey\n",
					"window_spec = Window.partitionBy(\"mve_gkey\").orderBy(F.col(\"t_put\").desc())\n",
					"df_latest = df_all.withColumn(\"rn\", F.row_number().over(window_spec)).filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
					"\n",
					"# Replace spaces with underscores for Delta\n",
					"for col_name in df_latest.columns:\n",
					"    df_latest = df_latest.withColumnRenamed(col_name, col_name.replace(\" \", \"_\"))"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"source": [
					"# from delta.tables import DeltaTable\n",
					"\n",
					"# # Path to your delta table\n",
					"# output_path = \"abfss://cmsa@navispowerbidatastorage2.dfs.core.windows.net/silver/moves\"\n",
					"\n",
					"# # Load table\n",
					"# delta_tbl = DeltaTable.forPath(spark, output_path)\n",
					"\n",
					"# # Current dataframe\n",
					"# df = delta_tbl.toDF()\n",
					"\n",
					"# # Drop the unwanted columns if they exist\n",
					"# cols_to_drop = {\"Hazardous_Flag\", \"Service_Id\"}\n",
					"# df_clean = df.drop(*[c for c in cols_to_drop if c in df.columns])\n",
					"\n",
					"# # Overwrite table with cleaned schema\n",
					"# df_clean.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(output_path)\n",
					"\n",
					"# print(\"âœ… Dropped missing cols and rewrote Delta table with clean schema.\")\n",
					"\n",
					"# # Get target schema\n",
					"# silver_delta = DeltaTable.forPath(spark, output_path)\n",
					"# target_cols = [f.name for f in silver_delta.toDF().schema.fields]\n",
					"# source_cols = df_latest.columns\n",
					"# miss_col = []\n",
					"# # Add missing cols with null\n",
					"# for col in target_cols:\n",
					"#     if col not in source_cols:\n",
					"#         miss_col.append(col)\n",
					"# print(miss_col)"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df_latest)"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\n",
					"\n",
					"if DeltaTable.isDeltaTable(spark, output_path):\n",
					"    silver_delta = DeltaTable.forPath(spark, output_path)\n",
					"    \n",
					"    (\n",
					"        silver_delta.alias(\"t\")\n",
					"        .merge(\n",
					"            df_latest.alias(\"s\"),\n",
					"            \"t.mve_gkey = s.mve_gkey\"\n",
					"        )\n",
					"        .whenMatchedUpdateAll()\n",
					"        .whenNotMatchedInsertAll()\n",
					"        .execute()\n",
					"    )\n",
					"    print(\"Merged\")\n",
					"else:\n",
					"    print(\"Overwriting...\")\n",
					"    # df_latest.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
					""
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}